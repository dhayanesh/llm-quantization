root@e1a1911bd0fe:/workspace# python inference.py
INFO 02-14 20:16:01 [__init__.py:216] Automatically detected platform cuda.
INFO 02-14 20:16:04 [utils.py:233] non-default args: {'disable_log_stats': True, 'model': 'ministral_8b-FP8-W8A8'}
INFO 02-14 20:16:04 [model.py:547] Resolved architecture: TransformersForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 02-14 20:16:04 [model.py:1510] Using max model len 32768
INFO 02-14 20:16:06 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
The tokenizer you are loading from 'ministral_8b-FP8-W8A8' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
(EngineCore_DP0 pid=3141) INFO 02-14 20:16:08 [core.py:644] Waiting for init message from front-end.
(EngineCore_DP0 pid=3141) INFO 02-14 20:16:08 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='ministral_8b-FP8-W8A8', speculative_config=None, tokenizer='ministral_8b-FP8-W8A8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ministral_8b-FP8-W8A8, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=3141) INFO 02-14 20:16:09 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3141) WARNING 02-14 20:16:09 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
(EngineCore_DP0 pid=3141) WARNING 02-14 20:16:10 [utils.py:184] TransformersForCausalLM has no vLLM implementation, falling back to Transformers implementation. Some features may not be supported and performance may not be optimal.
(EngineCore_DP0 pid=3141) INFO 02-14 20:16:10 [gpu_model_runner.py:2602] Starting to load model ministral_8b-FP8-W8A8...
(EngineCore_DP0 pid=3141) INFO 02-14 20:16:10 [gpu_model_runner.py:2634] Loading model from scratch...
(EngineCore_DP0 pid=3141) INFO 02-14 20:16:10 [transformers.py:442] Using Transformers backend.
(EngineCore_DP0 pid=3141) `torch_dtype` is deprecated! Use `dtype` instead!
(EngineCore_DP0 pid=3141) INFO 02-14 20:16:10 [cuda.py:366] Using Flash Attention backend on V1 engine.
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:22<00:22, 22.61s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:58<00:00, 30.71s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:58<00:00, 29.49s/it]
(EngineCore_DP0 pid=3141) 
(EngineCore_DP0 pid=3141) INFO 02-14 20:17:10 [default_loader.py:267] Loading weights took 59.24 seconds
(EngineCore_DP0 pid=3141) WARNING 02-14 20:17:10 [marlin_utils_fp8.py:80] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
(EngineCore_DP0 pid=3141) INFO 02-14 20:17:11 [gpu_model_runner.py:2653] Model loading took 8.4722 GiB and 59.828268 seconds
(EngineCore_DP0 pid=3141) INFO 02-14 20:17:20 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/b736ac8edc/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=3141) INFO 02-14 20:17:20 [backends.py:559] Dynamo bytecode transform time: 9.53 s
(EngineCore_DP0 pid=3141) INFO 02-14 20:17:26 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.232 s
(EngineCore_DP0 pid=3141) INFO 02-14 20:17:28 [monitor.py:34] torch.compile takes 9.53 s in total
(EngineCore_DP0 pid=3141) INFO 02-14 20:17:32 [gpu_worker.py:298] Available KV cache memory: 30.21 GiB
(EngineCore_DP0 pid=3141) INFO 02-14 20:17:32 [kv_cache_utils.py:1087] GPU KV cache size: 219,984 tokens
(EngineCore_DP0 pid=3141) INFO 02-14 20:17:32 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 6.71x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|████████████████████████████████████████████| 67/67 [00:07<00:00,  8.76it/s]
Capturing CUDA graphs (decode, FULL): 100%|███████████████████████████████████████████████████████████████| 35/35 [00:02<00:00, 12.98it/s]
(EngineCore_DP0 pid=3141) INFO 02-14 20:17:43 [gpu_model_runner.py:3480] Graph capturing finished in 11 secs, took 0.82 GiB
(EngineCore_DP0 pid=3141) INFO 02-14 20:17:43 [core.py:210] init engine (profile, create kv cache, warmup model) took 32.91 seconds
(EngineCore_DP0 pid=3141) The tokenizer you are loading from 'ministral_8b-FP8-W8A8' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
INFO 02-14 20:17:45 [llm.py:306] Supported_tasks: ['generate']
Model loaded in 100.42 seconds
Adding requests: 100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 173.71it/s]
Processed prompts: 100%|███████████████████████████████| 1/1 [00:04<00:00,  4.54s/it, est. speed input: 2.65 toks/s, output: 56.46 toks/s]
Adding requests: 100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 679.90it/s]
Processed prompts: 100%|███████████████████████████████| 1/1 [00:04<00:00,  4.52s/it, est. speed input: 2.66 toks/s, output: 56.71 toks/s]
Adding requests: 100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 736.23it/s]
Processed prompts: 100%|███████████████████████████████| 1/1 [00:04<00:00,  4.50s/it, est. speed input: 2.67 toks/s, output: 56.90 toks/s]
Adding requests: 100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 718.08it/s]
Processed prompts: 100%|███████████████████████████████| 1/1 [00:04<00:00,  4.51s/it, est. speed input: 2.66 toks/s, output: 56.80 toks/s]
Adding requests: 100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 718.69it/s]
Processed prompts: 100%|███████████████████████████████| 1/1 [00:04<00:00,  4.50s/it, est. speed input: 2.67 toks/s, output: 56.91 toks/s]
Adding requests: 100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 796.94it/s]
Processed prompts: 100%|███████████████████████████████| 1/1 [00:04<00:00,  4.50s/it, est. speed input: 2.67 toks/s, output: 56.94 toks/s]

=== Output ===
 Here are some reasons:

1. **Parallel Processing**: GPUs are designed to perform many operations in parallel, which is perfect for deep learning tasks. Deep learning models involve a large number of matrix multiplications and convolutions, which can be parallelized efficiently on GPUs.

2. **High Throughput**: GPUs can perform a large number of operations per second, making them ideal for training deep learning models. This high throughput allows for faster training times, which is crucial for large-scale models.

3. **Memory Bandwidth**: GPUs have high memory bandwidth, which means they can quickly read and write data to memory. This is important for deep learning tasks, which often involve large datasets and complex models.

4. **Specialized Hardware**: GPUs are designed with specialized hardware for tasks like matrix multiplications and convolutions, which are common in deep learning. This specialized hardware allows GPUs to perform these tasks more efficiently than general-purpose CPUs.

5. **Scalability**: GPUs can be used in parallel to train models, allowing for even faster training times. This scalability is particularly useful for large-scale deep learning tasks.

6. **Energy Efficiency**: While GPUs consume a lot of power, they are also very energy-efficient for the amount of work

Generation time: 22.53 seconds
